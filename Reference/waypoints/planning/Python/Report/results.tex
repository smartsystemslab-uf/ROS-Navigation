\section{Results}
All results were simulated in Python using the networkx graph library. Simulation was performed on Ubuntu 16.04 using an i7 quad core processor.

All results in the pathing section were simulated on a variety of graphs. Parent grid sizes of 5x5, 10x10, 20x20, 50x50, and 100x100 were used. The rates of connectivity used on the child layer were 25\%, 50\%, and 75\%. Every permutation of grid size and connectivity rate were used giving a total of 15 total test cases. In each case a random graph was generated using a connectivity rate of 80\% on the parent layer. The child layer for each parent was given a random number of child nodes from 5 to 10. 1000 paths were run with each permutation giving a total of 15000 paths run each for Greedy, D-A* V1, and D-A* V2. This section compares the requests made and runtime of the two versions of D-A*. The cost is not compared as the cost is equal between the two methods as expected. The results of D-A* V2 are then compared to Greedy. In this case, cost is considered. D-A* V1 is not used in this comparison as V2 is nearly a complete improvement over V1.

For Multi-Agent testing, the same conditions apply. However, the 100x100 grid was omitted and only 100 paths were run for each permutation. This section will compare the times required to run paths serially vs in parallel using the multithreading method described in 5.1. Only D-A* V2 was used in this test. Since the goal of these results is to only compare runtime between the serial and multithreaded version of the same algorithm, only the most effective algorithm was tested.


I believe this grid size is a valid representation of the potential real system. Based on the assumption that a camera can see a 10x10 foot region from the ceiling of a warehouse with 1 foot of overlap, the viewable region is over 800,000 ft$^2$.

This section is going to focus on the real value results to show how the values scale with grid size and connectivity rate. Figures showing normalized value comparisons will be added into the Appendix.

\subsection{Pathing}
	\subsubsection{D-A* Comparison}
	In Figures \ref{fig:DAScompare25}, \ref{fig:DAScompare50}, and \ref{fig:DAScompare75}, the request count and runtime D-A* V1 and V2 are compared for all grid sizes with a connectivity rate of 25\%, 50\%, and 75\% respectively.
	
	As shown in the figures, the connectivity of the graph does not have much of an effect on the relative values of the two versions. While they do differ slightly, it's insignificant. The significance of these results lies in the comparison between the two versions at any connectivity. 
	
	At each grid size, the requests made by V2 is about 60\% of V1. It can also be seen that the runtime difference between the two versions is extremely small. The most significant difference sits at grid size 100 whose difference is slightly over 0.01 seconds on average. It should be noted that in this simulation, a request was not run through a network as I had no physical system. Requests were simulated by forcing the node being requested to call the corresponding function. Therefore, it is possible that the substantial difference in network requests will cause V1 to be slower than V2 in a real environment. Even if the request time of any average request is low enough to maintain a shorter runtime, the rate of requests may delay responses enough to tip runtime to the favor of V2.	
	
	It was mentioned in 4.2.2 that 66\% request reduction could not be achieved. Requests are made in other portions of the implementation, such as rebuilding the path, which are unaffected by the change made in V2. 
		\input{resultsfigs/real/DAStarComp25.tex}
		\input{resultsfigs/real/DAStarComp50.tex}
		\input{resultsfigs/real/DAStarComp75.tex}
	\subsubsection{D-A* vs Greedy}
	
	Due to the scale of difference between values generated by Greedy and D-A*, this section will focus on normalized results. In these results, D-A* values have been normalized to Greedy. Therefore, Greedy values are not represented on the charts as they are always one. Figures \ref{fig:DAGreedyPathLength}, \ref{fig:DAGreedyRequests}, and \ref{fig:DAGreedyRuntime} show the comparison between path length, request count, and runtime respectively.
	
	The results show that D-A* takes magnitudes longer to run than Greedy. This is expected as Greedy goes directly for the goal with no checks. It has neither the processing nor spread on the grid of D-A*, so it is expected that it runs much faster. For the same reason, the number of requests made by D-A* is up to 27 times the requests made by Greedy.
	
	Path length is fairly consistent across grid sizes. It does have a slight decline and reaches under 0.8 in one instance. 
	
	As stated in section 4.2.1, greedy sometimes outperforms D-A* in terms of path length. Out of the 15000 total paths generated, Greedy had a lower cost in 364 paths for a total of 2.42\%. While this number is very small, it is important to know what the average difference was. In Figure \ref{fig:GreedyOutperform}, the average path length of D-A* paths which were longer than Greedy are shown normalized to Greedy's path length. Based on the figure, there seems to be a slight downward trend for low and high connectivity as grid size increases. In the worst case, D-A* was 6.8\% longer; in the best case, 1.7\% longer. As the size of the area increases, this difference becomes more substantial in regard to real path length. Because the runtime of Greedy is magnitudes lower than D-A*, it would be possible to run both methods and take the shorter path.
	
	It should be noted that the "total" memory used in this system is higher than performing A* across the bottom layer as if each region were connected to its neighbors. Determining which path to take across a region requires knowing which of the available paths are the shortest; It is necessary to run a shortest path algorithm to determine this. 
	
In a static system, the paths through each node can be cached with much higher practicality than the paths across the entire child layer. Therefore, it is likely that this method would still outperform a non-hierarchical implementation in terms of memory used at runtime. 

However, in dynamic environment, especially one in which path costs are constantly changing, calculation of cost requires running a shortest path algorithm on the child layer. In this case, we are effectively pathing across the entire child layer as if it is not hierarchical. Due to the hierarchical nature of the implementation, there are some nodes that would be left unexpanded. The goal being found and verified as most optimal could leave some parent nodes unexpanded whose children would otherwise be expanded in a non-hierarchical implementation. However as this would not be the majority of the graph, we are still using more memory by pathing across both layers. If this system were centralized, this would be a real problem. 

However, the decentralized nature of this system renders the problem moot. This system has no concept of "total memory" as each node is a separate entity acting independently of the rest of the network. While the "total memory" used is higher, the memory used on any given node is much lower. The source node uses the most memory by pathing across its child layer while also managing the queue and sets required to path across the parent layer. With the decentralized implementation, the number of nodes expanded in the parent layer is equal to the centralized implementation. Therefore, we can ignore the parent layer in this memory comparison.

With the disregard of the parent layer, the memory of the centralized system becomes the entirety of the expanded set of nodes on the child layer. The memory of the decentralized system is then divided into the individual nodes used. As each node only handles a small subset of points on the child layer, the memory of a single node in this graph is greatly reduced to about $\frac{C}{N}$ where $N$ is the number of expanded nodes in the parent layer and $C$ is the number of nodes expanded in the child layer. Thereofre, I claim that this increase in total memory is not a true problem in this system.

	\input{resultsfigs/normalized/DAGreedyRuntime.tex}
	\input{resultsfigs/normalized/GreedyOutperform.tex}
\subsection{Multi-Agent}
Multi-Agent did not pan out as initially expected. The run time with multithreading is worse than running in serial. The reason for this is described in Section 5.2. 

The runtime comparison results are shown in Figure \ref{fig:MTCompare}. Looking at the figure, it can be seen that the runtime of the multithreaded implementation climbs quickly as the size of the grid grows. This rapid growth is the reason grid size 100 is not represented in these results. The reason why this happens was discussed in Section 5.2.

Methods of potentially improving the runtime of the multithreaded implementation are discussed in Section 7.2.

\input{resultsfigs/real/MT.tex}